<script setup lang="ts">
import Codeblock from '@/components/Codeblock.vue'
import Note from '@/components/Note.vue'
import Article from '@/components/Article.vue'
import External from '@/components/External.vue'
import Formula from '@/components/Formula.vue'
import SvgBehindImage from '@/components/SvgBehindImage.vue'
import VideoBlock from '@/components/VideoBlock.vue'


const navItems = [
  {
    href: '#top',
    label: 'Top',
  },
]

</script>

<template>
  <Article :items="navItems" back="/blog" id="top">

    <p class="text-3xl">Focal Generation</p>

    <p>Focal generation is a term I coined for the process of supplementing a larger model using a smaller model in
      order to save computational resources. In this post I will focus on the text generation usecase. The way it works
      is that there are three models: the small model, the large model, and the focus model. The large model is
      computationally expensive, but performs well. The small model in computationally inexpensive, but performs worse
      that the large model. The focal model is responsible for choosing which model to use to generate the next token.
    </p>

    <p>The key challenge for the focal model is to be able to discern places within the output sequence where the small
      model performs as well or almost as well as the large model, and therefore computational resources can be saved by
      avoiding using the large model to generate those tokens. How can we train the focal model for this task?</p>

    <p>I will make two proposals for how to train the focal model. The first is training using reinforcement learning,
      where the observation is a sequence of output tokens
      generated by the large model, and the action space is only two actions: "use large model" or "use small model".
      There are many reward functions that could work, here is one example:</p>


    <p>The focal model gains 1 reward whenever it uses the small model, and the token generated by the small model is
      exactly the same as the large model. It also gains 1 reward whenever it uses the large model, and the small model
      would have output something other than the large model. In all other cases, it loses 1 reward. See the table
      below:</p>

    <div class="grid grid-cols-3 gap-2 w-fit mx-auto px-2 py-1 border border-gray-400 text-center">
      <div></div>
      <div>Use Small</div>
      <div>Use Large</div>
      <div>Same token</div>
      <div>1</div>
      <div>-1</div>
      <div>Wrong token</div>
      <div>-1</div>
      <div>1</div>
    </div>

    <p>A second way to train the focal model is to use adversarial training. A fourth model (the discriminator) is
      introduced, with its task being to distinguish text generated by the focal model v.s. the large model. The focal
      model is trained the same way as before, except this time it doesn't have to match the tokens generated by the
      large model exactly, instead it must only fool the discriminator.</p>

    <div class="grid grid-cols-3 gap-2 w-fit mx-auto px-2 py-1 border border-gray-400 text-center">
      <div></div>
      <div>Use Small</div>
      <div>Use Large</div>
      <div>Disriminator wrong</div>
      <div>1</div>
      <div>-1</div>
      <div>Discriminator correct</div>
      <div>-1</div>
      <div>1</div>
    </div>

    <p class="text-xl mt-8" id="what-is-ppo">Code example</p>

    <p>TODO</p>

    <p class="my-16 py-16"></p>


  </Article>
</template>